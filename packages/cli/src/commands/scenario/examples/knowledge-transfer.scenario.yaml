name: "Knowledge Transfer Effectiveness Test"
description: "Test agent's ability to teach complex concepts through multi-turn conversation"

plugins:
  - name: '@elizaos/plugin-bootstrap'
    enabled: true
  - name: '@elizaos/plugin-sql'
    enabled: true
  - name: '@elizaos/plugin-openai'
    enabled: true

environment:
  type: local

run:
  - name: "Teaching machine learning concepts to beginner"
    input: "Can you explain how machine learning works? I'm not very technical but I'm curious"
    
    conversation:
      max_turns: 10
      timeout_per_turn_ms: 35000
      
      user_simulator:
        model_type: "TEXT_LARGE"
        temperature: 0.7
        max_tokens: 250
        persona: "curious non-technical professional who wants to understand ML basics"
        objective: "gain practical understanding of machine learning concepts and applications"
        style: "eager to learn but needs simple explanations and concrete examples"
        constraints:
          - "Ask clarifying questions when concepts seem abstract"
          - "Request simpler explanations for complex technical terms"
          - "Give feedback on understanding level ('I think I get it' or 'I'm still confused')"
          - "Ask for practical, real-world examples"
          - "Show appreciation for clear explanations with analogies"
          - "Ask follow-up questions about applications in their field"
        emotional_state: "curious and motivated but slightly intimidated by technical topics"
        knowledge_level: "beginner"
      
      termination_conditions:
        - type: "user_expresses_satisfaction"
          keywords: ["I understand now", "that makes sense", "clear explanation", "thank you"]
        - type: "custom_condition"
          llm_judge:
            prompt: "Does the user demonstrate a solid basic understanding of machine learning concepts?"
            threshold: 0.8
      
      turn_evaluations:
        - type: "llm_judge"
          prompt: "Was the explanation appropriate for a beginner's level?"
          expected: "yes"
        - type: "llm_judge"
          prompt: "Did the agent use helpful analogies or concrete examples?"
          expected: "yes"
        - type: "llm_judge"
          prompt: "Did the agent check for understanding before moving to next concept?"
          expected: "yes"
      
      final_evaluations:
        - type: "llm_judge"
          prompt: "Did the agent successfully teach machine learning concepts to a non-technical user?"
          expected: "yes"
          capabilities:
            - "Adapted language and explanations to beginner level"
            - "Used concrete examples and analogies to explain abstract concepts"
            - "Checked understanding regularly throughout conversation"
            - "Built concepts progressively from simple to more complex"
            - "Encouraged questions and created safe learning environment"
            - "Provided practical applications relevant to user's interests"
        - type: "conversation_length"
          min_turns: 6
          max_turns: 12
          optimal_turns: 8
        - type: "conversation_flow"
          required_patterns:
            - "question_then_answer"
            - "clarification_cycle"
          flow_quality_threshold: 0.8
        - type: "user_satisfaction"
          satisfaction_threshold: 0.8
          measurement_method: "llm_judge"
        - type: "context_retention"
          test_memory_of: ["machine learning", "beginner level", "non-technical", "practical examples"]
          retention_turns: 6
          memory_accuracy_threshold: 0.85

    evaluations:
      - type: "string_contains"
        value: "machine learning"
      - type: "llm_judge"
        prompt: "Was the explanation educational and appropriate for the audience?"
        expected: "yes"

judgment:
  strategy: all_pass
